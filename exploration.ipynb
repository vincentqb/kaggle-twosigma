{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Profiling notebook perfomance\n",
    "from time import clock\n",
    "start_notebook = clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open data file\n",
    "df = pd.read_hdf('data/train.h5')\n",
    "# df.set_index('id', inplace=True)\n",
    "\n",
    "# Subsample for now...\n",
    "df = df[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.to_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excl = ['id', 'sample', 'y', 'timestamp']\n",
    "cols = [c for c in df.columns if c not in excl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df.columns)\n",
    "\n",
    "from collections import Counter\n",
    "col_prefix = [col.split('_')[0] for col in df.columns]\n",
    "counter = Counter(col_prefix)\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.set_index('timestamp')['fundamental_0'].plot()\n",
    "# df.set_index('timestamp')['derived_0'].plot()\n",
    "# df.set_index('timestamp')['technical_0'].plot()\n",
    "# df.set_index('timestamp')['derived_3'].plot()\n",
    "# df.set_index('timestamp')['technical_41'].plot()\n",
    "# df.set_index('fundamental_0')['fundamental_1'].plot()\n",
    "\n",
    "# Seasonal pattern?\n",
    "# series = df.set_index('timestamp')['fundamental_0'].ffill()\n",
    "# series = series.rolling(window=1000).mean()\n",
    "# series.plot()\n",
    "\n",
    "# Distribution of target in time\n",
    "df.plot.hexbin('timestamp', 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[['timestamp', 'fundamental_0', 'derived_0', 'technical_0']].dropna().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.loc[df['id'] == 10, 'y'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[df['id'] == 41, 'y'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look a differences, as [anokas](https://www.kaggle.com/anokas/two-sigma-financial-modeling/two-sigma-time-travel-eda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How does the number of timestamps evolve?\n",
    "diff = df.groupby('timestamp')['timestamp'].count().diff()\n",
    "diff.plot()\n",
    "\n",
    "# What is the frequency of the large peaks?\n",
    "pd.Series(diff[diff > 10].index).diff()\n",
    "print(diff[diff > 10].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count unique per columns\n",
    "# nuniq = df.apply(pd.Series.nunique)\n",
    "# nuniq = df.apply(lambda x: len(x.unique()))  # faster?\n",
    "# print(nuniq)\n",
    "\n",
    "# Round number before counting\n",
    "# df.apply(lambda x: round(x, 3)).nunique()\n",
    "\n",
    "# Count number of unique per column\n",
    "# df[['fundamental_0', 'derived_0', 'technical_0']].apply(pd.Series.nunique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As done by [sudalairajkumar](https://www.kaggle.com/sudalairajkumar/two-sigma-financial-modeling/simple-exploration-notebook), we notice a clear period in the timestamp distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['timestamp'].astype('int').value_counts().sort_index().diff().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_bins = 300\n",
    "df['timestamp'].hist(bins=n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df['timestamp'].value_counts().plot.barh()\n",
    "# df['timestamp'].value_counts().sort_index().diff().plot()\n",
    "\n",
    "dft = df[['timestamp']]\n",
    "dft['bin'], bins = pd.cut(df[['timestamp']], n_bins, labels=False, retbins=True)\n",
    "# half-open intervals for each bin: (bins[0], bins[1]]\n",
    "\n",
    "dft_diff = dft.groupby('bin').count().diff()\n",
    "dft_diff.plot()\n",
    "\n",
    "dft_diff[dft_diff['timestamp'] > 50].index\n",
    "# The spikes are very regular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As [sudalairajkumar](https://www.kaggle.com/sudalairajkumar/two-sigma-financial-modeling/univariate-analysis-regression-lb-0-006) did, we can look at the correlation of the columns with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Correlation?\n",
    "corr = df[cols].corrwith(df['y'], drop=True)\n",
    "abs(corr).plot.barh(figsize=(6,15))\n",
    "\n",
    "cols_corr = corr[abs(corr) > 0.005]\n",
    "print(cols_corr)\n",
    "\n",
    "cols_corr = cols_corr.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of missing values\n",
    "n = df.shape[0]\n",
    "nas = df.isnull().sum()/n\n",
    "nas.plot.barh(figsize=(6,15))\n",
    "\n",
    "print(\"total: {:.0%}\".format(nas.mean()))\n",
    "\n",
    "# Drop columns with too many missing values\n",
    "cols_na = df.columns[nas > 0.3]\n",
    "\n",
    "print(cols_na)\n",
    "df.drop(cols_na, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the distribution, as done by [wangruixin](https://www.kaggle.com/wangruixin/two-sigma-financial-modeling/randomforestregressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def remove_outliers(col):\n",
    "    \"\"\"Remove outliers from column.\"\"\"\n",
    "    \n",
    "    # Ignore missing values\n",
    "    col = col.dropna()\n",
    "    \n",
    "    # First quantile\n",
    "    q_low = col.quantile(.25)\n",
    "    q_high = col.quantile(.75)\n",
    "    q_diff = q_high - q_low\n",
    "    \n",
    "    # Add buffer to quantile\n",
    "    low = q_low - 1.5 * q_diff\n",
    "    high = q_high + 1.5 * q_diff\n",
    "    \n",
    "    # Drop values outside range\n",
    "    col[(col > high) | (col < low)] = np.nan\n",
    "    \n",
    "    return col\n",
    "\n",
    "# Find and remove outliers in target\n",
    "# outliers = find_outliers(target)\n",
    "# obs.drop(outliers, inplace=True)\n",
    "# target.drop(outliers, inplace=True)\n",
    "\n",
    "# Plot histogram after removing outliers\n",
    "df.apply(remove_outliers).hist(\n",
    "    layout=(-1, 4), figsize=(10, 60), bins=20, sharex=False, sharey=False\n",
    ")\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Columns with thin histograms\n",
    "cols_one = ['technical_13', 'technical_16', 'technical_18', 'technical_20', 'technical_30',\n",
    "            'technical_42', 'technical_9', 'technical_0', 'technical_12', 'technical_37',\n",
    "            'technical_38', 'technical_39']\n",
    "cols_two = ['technical_10', 'technical_29', 'technical_14', 'technical_43', 'technical_6']\n",
    "cols_three = ['technical_22', 'technical_34']\n",
    "# Could also do some clustering instead\n",
    "\n",
    "# Remove columns with one category\n",
    "df.drop(cols_one, inplace=True, axis=1)\n",
    "\n",
    "# One-hot encode categories\n",
    "df[cols_two] = df[cols_two].apply(lambda x: x > -1)\n",
    "\n",
    "# One-hot encode categories\n",
    "for c in cols_three:\n",
    "    df[c + '_A'] = df[c] > 2.5\n",
    "    df[c + '_B'] = df[c] < -1.5\n",
    "df.drop(cols_three, inplace=True, axis=1)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_outliers(col):\n",
    "    \"\"\"Find outliers in column.\"\"\"\n",
    "\n",
    "    # Ignore missing values\n",
    "    col = col.dropna()\n",
    "\n",
    "    # First quantile\n",
    "    q_low = col.quantile(.25)\n",
    "    q_high = col.quantile(.75)\n",
    "    q_diff = q_high - q_low\n",
    "\n",
    "    # Add buffer to quantile\n",
    "    low = q_low - 1.5 * q_diff\n",
    "    high = q_high + 1.5 * q_diff\n",
    "    \n",
    "    print(low, high)\n",
    "\n",
    "    # Drop values outside range\n",
    "    outliers = (col > high) | (col < low)\n",
    "\n",
    "    return df[outliers].index\n",
    "\n",
    "# Find outliers in target\n",
    "outliers = find_outliers(df['y'])\n",
    "print(len(outliers))\n",
    "\n",
    "# Remove outliers\n",
    "df.drop(outliers, inplace=True)\n",
    "\n",
    "# Record min/max at this point, useful for clipping\n",
    "y_max = max(df['y'])\n",
    "y_min = min(df['y'])\n",
    "\n",
    "df.plot.hexbin('timestamp', 'y')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clip(col):\n",
    "    \"\"\"Clip values outside of [y_min, y_max].\"\"\"\n",
    "    too_high = col > y_max\n",
    "    col[too_high] = y_max\n",
    "    too_low = col < y_min\n",
    "    col[too_low] = y_min\n",
    "    return col\n",
    "\n",
    "print(y_min, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "\n",
    "# df = df.sort_values(by='id')\n",
    "# df = df.sort_values(by='timestamp')\n",
    "# df = df.sort_values(by='y')  # Assume similarity between nearby targets\n",
    "\n",
    "# df = df.fillna(method='ffill')\n",
    "# df = df.fillna(method='bfill')\n",
    "        \n",
    "# mean_values = df.mean(axis=0)\n",
    "mean_values = df.median(axis=0)  # more robust to outliers\n",
    "df.fillna(mean_values, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some rows are all missing values in test set\n",
    "# https://www.kaggle.com/c/two-sigma-financial-modeling/discussion/26205\n",
    "df.dropna(how='all', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind = 'timestamp'\n",
    "\n",
    "target = 'y'\n",
    "cols = [c for c in df.columns if c not in excl]\n",
    "# cols = ['fundamental_17', 'fundamental_41', 'technical_19', 'fundamental_62', 'fundamental_48']\n",
    "cols = [c for c in df.columns if c not in excl and c in cols_corr]\n",
    "\n",
    "target = df.set_index(ind)[target]\n",
    "feature = df.set_index(ind)[cols]\n",
    "\n",
    "print(feature.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_train_test(feature, target, cutoff_test = 1400):\n",
    "    \"\"\"\n",
    "    Divide features and targets into train and test\n",
    "    \"\"\"\n",
    "\n",
    "    ind_test = df.index >= cutoff_test\n",
    "    feature_test = feature[ind_test]\n",
    "    target_test = target[ind_test]\n",
    "\n",
    "    ind_train = ~ind_test\n",
    "    feature_train = feature[ind_train]\n",
    "    target_train = target[ind_train]\n",
    "    \n",
    "    return feature_train, feature_test, target_train, target_test\n",
    "\n",
    "# Apply split\n",
    "feature_train, feature_test, target_train, target_test = split_train_test(feature, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "\n",
    "# LassoCV since L1 norm promotes sparsity of features\n",
    "clf = LassoCV()\n",
    "# clf = LassoCV(alphas=[0.01, 0.1, 0.5, 1.])\n",
    "# clf = Lasso(alpha=0.01)\n",
    "# sfm = SelectFromModel(clf, threshold = 1e-7)\n",
    "sfm = SelectFromModel(clf, threshold = \"mean\")\n",
    "sfm.fit(feature_train, target_train)\n",
    "# NOTE had to disable mkl as discussed here: https://github.com/BVLC/caffe/issues/3884\n",
    "\n",
    "feature_kept = feature.columns[sfm.get_support()]\n",
    "print(\"Features: {}\".format(feature_kept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep only most important features\n",
    "# feature_train = pd.DataFrame(sfm.transform(feature_train), \n",
    "#                              columns = feature_kept, index = feature_train.index)\n",
    "# feature_test = pd.DataFrame(sfm.transform(feature_test), \n",
    "#                             columns = feature_kept, index = feature_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# LassoCV since L1 norm promotes sparsity of features\n",
    "reg = LassoCV(alphas=[0.001, 0.01, 0.1, 0.5, 1., 10, 100])\n",
    "reg.fit(feature_train, target_train)\n",
    "\n",
    "print(reg.alpha_)\n",
    "print(reg.coef_)\n",
    "print(reg.mse_path_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# LassoCV since L1 norm promotes sparsity of features\n",
    "reg = LassoCV(alphas=[0.001, 0.01, 0.1, 0.5, 1., 10, 100, 10**3, 10**5])\n",
    "reg.fit(feature_train, target_train)\n",
    "\n",
    "print(reg.alpha_)\n",
    "print(reg.coef_)\n",
    "print(reg.mse_path_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# LassoCV since L1 norm promotes sparsity of features\n",
    "reg = ElasticNetCV(\n",
    "    alphas=[0.001, 0.01, 0.1, 0.5, 1., 10, 100, 10**3, 10**5],\n",
    "    l1_ratio = [0., 0.01, 0.1, 0.5, 0.8, 0.95, 0.99, 1.]\n",
    ")\n",
    "reg.fit(feature_train, target_train)\n",
    "\n",
    "print(reg.alpha_)\n",
    "print(reg.l1_ratio_)\n",
    "print(reg.coef_)\n",
    "print(reg.mse_path_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg = ElasticNetCV(alphas=[0.001, 0.01, 0.1, 0.5, 1.], l1_ratio=[0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "# reg = LinearRegression()\n",
    "\n",
    "reg = ElasticNet(alpha=0.01, l1_ratio=0.1)\n",
    "\n",
    "# Quick cross validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(reg, feature, target, cv = 5)\n",
    "print(\"R^2 during CV: {:.2f} +/- {:.2f}\".format(scores.mean(), scores.std() * 2))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "reg = RandomForestRegressor()\n",
    "tuned_params = {'n_estimators': [100, 500, 1000], 'max_depth': [None, 1, 2, 5, 10], 'min_samples_split': [1, 2, 3]}\n",
    "# reg = ExtraTreesRegressor()\n",
    "# reg = ExtraTreesRegressor(n_estimators=100, max_depth=4)\n",
    "\n",
    "gs = GridSearchCV(reg, tuned_params, cv=5, n_jobs=-1, verbose=1)\n",
    "gs.fit(feature_train, target_train)\n",
    "\n",
    "print(gs.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg = ElasticNet(alpha=0.01, l1_ratio=0.1)\n",
    "\n",
    "reg.fit(feature_train, target_train)\n",
    "\n",
    "# Score on train/test set\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "pred_train = reg.predict(feature_train)\n",
    "# pred_train = clip(pred_train)\n",
    "print('train score: {}'.format(r2_score(target_train, pred_train)))\n",
    "\n",
    "pred_test = reg.predict(feature_test)\n",
    "# pred_test = clip(pred_test)\n",
    "print('test score: {}'.format(r2_score(target_test, pred_test)))\n",
    "\n",
    "# Big difference! Overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = pd.DataFrame({'target': target_test.values, 'predicted': pred_test, 'timestamp': target_test.index})\n",
    "\n",
    "r.plot.hexbin('timestamp', 'target')\n",
    "r.plot.hexbin('timestamp', 'predicted')\n",
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "# r.plot.hexbin('timestamp', 'target', ax=axes[0], sharex=False, sharey=True, legend=False)\n",
    "# r.plot.hexbin('timestamp', 'predicted', ax=axes[1], sharex=False, sharey=True, legend=False)\n",
    "\n",
    "# plt.subplots_adjust(wspace=0.5, hspace=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    pd.DataFrame(reg.feature_importances_, index=feature_train.columns).plot.barh(figsize=(6,10))\n",
    "except AttributeError:\n",
    "    pass\n",
    "cols_important = ['fundamental_17', 'fundamental_41', 'technical_19', 'fundamental_62', 'fundamental_48']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(outcome, predict):\n",
    "    \"\"\"\n",
    "    Compute Mean Absolute Percentage Error (MAPE) score. Positive, but lower is better.\n",
    "    \"\"\"\n",
    "    \n",
    "    outcome = np.array(outcome).ravel()\n",
    "    predict = np.array(predict).ravel()\n",
    "    \n",
    "    # Get only the NONZERO or NON-NAN elements\n",
    "    EPSILON = pow(10, -5)\n",
    "    idx = (np.abs(outcome) > EPSILON) | (~np.isnan(outcome)) | (~np.isnan(predict))\n",
    "    \n",
    "    # Extract those elements\n",
    "    outcome = outcome[np.where(idx)]\n",
    "    predict = predict[np.where(idx)]\n",
    "    \n",
    "    return np.mean(np.abs((outcome - predict) / outcome))\n",
    "\n",
    "scores = {}\n",
    "\n",
    "scores['MAPE'] = mape(target_test, pred_test)\n",
    "        \n",
    "from sklearn.metrics import r2_score\n",
    "scores['R2'] = r2_score(target_test, pred_test)\n",
    "\n",
    "from sklearn.metrics import explained_variance_score\n",
    "scores['Explained Variance'] = explained_variance_score(target_test, pred_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "scores['Mean Square Error'] = mean_squared_error(target_test, pred_test)\n",
    "scores['Root Mean Square Error'] = np.sqrt(scores['Mean Square Error'])\n",
    "    \n",
    "from sklearn.metrics import median_absolute_error\n",
    "scores['Median Absolute Error'] = median_absolute_error(target_test, pred_test)\n",
    "\n",
    "print(pd.Series(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining models\n",
    "\n",
    "We can combine Trees with Linear Regression, as done by [dikshant2210](https://www.kaggle.com/dikshant2210/two-sigma-financial-modeling/winter/run/652355)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Notebook ran in {:.1f} minutes\".format((clock() - start_notebook)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
