{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Profiling notebook perfomance\n",
    "from time import clock\n",
    "start_notebook = clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_hdf('data/train.h5')\n",
    "df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "excl = ['id', 'sample', 'y', 'timestamp']\n",
    "cols = [c for c in df.columns if c not in excl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df.columns)\n",
    "\n",
    "from collections import Counter\n",
    "col_prefix = [col.split('_')[0] for col in df.columns]\n",
    "counter = Counter(col_prefix)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[['fundamental_0', 'derived_0', 'technical_0']].dropna().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count unique per columns\n",
    "# nuniq = df.apply(pd.Series.nunique)\n",
    "# print(nuniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count number of unique per column\n",
    "# df[['fundamental_0', 'derived_0', 'technical_0']].apply(pd.Series.nunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of missing values\n",
    "n = df.shape[0]\n",
    "nas = df.isnull().sum()/n\n",
    "print(\"total: {:.0%}\".format(nas.mean()))\n",
    "\n",
    "print(nas * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target = df['y']\n",
    "# df = df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.set_index('timestamp')['fundamental_0'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.set_index('timestamp')['derived_0'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.set_index('timestamp')['technical_0'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.set_index('timestamp')['derived_3'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.set_index('timestamp')['technical_41'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.set_index('fundamental_0')['fundamental_1'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Seasonal pattern?\n",
    "series = df.set_index('timestamp')['fundamental_0'].ffill()\n",
    "series = series.rolling(window=1000).mean()\n",
    "series.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = df['y']\n",
    "feature = df[cols].ffill().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_train_test(feature, target, cutoff_test = 1000):\n",
    "    \"\"\"\n",
    "    Divide features and targets into train and test\n",
    "    \"\"\"\n",
    "\n",
    "    ind_test = df.index >= cutoff_test\n",
    "    feature_test = feature[ind_test]\n",
    "    target_test = target[ind_test]\n",
    "\n",
    "    ind_train = ~ind_test\n",
    "    feature_train = feature[ind_train]\n",
    "    target_train = target[ind_train]\n",
    "    \n",
    "    return feature_train, feature_test, target_train, target_test\n",
    "\n",
    "# Apply split\n",
    "feature_train, feature_test, target_train, target_test = split_train_test(feature, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# LassoCV since L1 norm promotes sparsity of features\n",
    "clf = LassoCV()\n",
    "# sfm = SelectFromModel(clf, threshold = 1e-7)\n",
    "sfm = SelectFromModel(clf, threshold = \"mean\")\n",
    "sfm.fit(feature_train, target_train)\n",
    "# NOTE had to disable mkl as discussed here: https://github.com/BVLC/caffe/issues/3884\n",
    "\n",
    "feature_kept = feature.columns[sfm.get_support()]\n",
    "print(\"Features: {}\".format(feature_kept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep only most important features\n",
    "feature_train = pd.DataFrame(sfm.transform(feature_train), \n",
    "                             columns = feature_kept, index = feature_train.index)\n",
    "feature_test = pd.DataFrame(sfm.transform(feature_test), \n",
    "                            columns = feature_kept, index = feature_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Quick cross validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(reg, feature, target, cv = 5)\n",
    "print(\"R^2 during CV: {:.2f} +/- {:.2f}\".format(scores.mean(), scores.std() * 2))\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg = RandomForestRegressor(n_estimators = 10)\n",
    "\n",
    "reg.train(feature_train, target_train)\n",
    "pred_test = reg.predict(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mape(outcome, predict):\n",
    "    \"\"\"\n",
    "    Compute Mean Absolute Percentage Error (MAPE) score. Positive, but lower is better.\n",
    "    \"\"\"\n",
    "    \n",
    "    outcome = np.array(outcome).ravel()\n",
    "    predict = np.array(predict).ravel()\n",
    "    \n",
    "    # Get only the NONZERO or NON-NAN elements\n",
    "    EPSILON = pow(10, -5)\n",
    "    idx = (np.abs(outcome) > EPSILON) | (~np.isnan(outcome)) | (~np.isnan(predict))\n",
    "    \n",
    "    # Extract those elements\n",
    "    outcome = outcome[np.where(idx)]\n",
    "    predict = predict[np.where(idx)]\n",
    "    \n",
    "    return np.mean(np.abs((outcome - predict) / outcome))\n",
    "\n",
    "scores['MAPE'] = mape(target_test, pred_test)\n",
    "        \n",
    "from sklearn.metrics import r2_score\n",
    "scores['R2'] = r2_score(target_test, pred_test)\n",
    "\n",
    "from sklearn.metrics import explained_variance_score\n",
    "scores['Explained Variance'] = explained_variance_score(target_test, pred_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "scores['Mean Square Error'] = mean_squared_error(target_test, pred_test)\n",
    "scores['Root Mean Square Error'] = np.sqrt(self.mse)\n",
    "    \n",
    "from sklearn.metrics import median_absolute_error\n",
    "scores['Median Absolute Error'] = median_absolute_error(target_test, pred_test)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(scores)\n",
    "# print(pd.Series(scores, name = 'Scores'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
